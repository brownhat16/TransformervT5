# Sparse Embedding Model Training with Sentence Transformers v5
# Complete Colab Notebook for Training and Fine-tuning Sparse Encoders

# =============================================================================
# SECTION 1: SETUP AND INSTALLATION
# =============================================================================

# Install required packages
!pip install -q sentence-transformers>=3.1.0
!pip install -q datasets
!pip install -q transformers
!pip install -q torch
!pip install -q huggingface_hub
!pip install -q wandb  # Optional: for experiment tracking
!pip install -q tensorboard  # Optional: for logging

# Import required libraries
import logging
import os
from typing import Optional, Dict, Any
import torch
from datasets import load_dataset, Dataset
from sentence_transformers import (
    SparseEncoder,
    SparseEncoderModelCardData,
    SparseEncoderTrainer,
    SparseEncoderTrainingArguments,
)
from sentence_transformers.models import Router
from sentence_transformers.sparse_encoder.evaluation import (
    SparseNanoBEIREvaluator,
    SparseEmbeddingSimilarityEvaluator
)
from sentence_transformers.sparse_encoder.losses import (
    SparseMultipleNegativesRankingLoss,
    SpladeLoss,
    CSRLoss,
    SparseCoSENTLoss
)
from sentence_transformers.sparse_encoder.models import (
    SparseStaticEmbedding,
    MLMTransformer,
    SpladePooling,
    SparseAutoEncoder
)
from sentence_transformers.training_args import BatchSamplers
from sentence_transformers.evaluation import SimilarityFunction
from huggingface_hub import login
import warnings
warnings.filterwarnings("ignore")

# Setup logging
logging.basicConfig(
    format="%(asctime)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO
)

print("‚úÖ All packages installed and imported successfully!")

# =============================================================================
# SECTION 2: CONFIGURATION
# =============================================================================

class TrainingConfig:
    """Configuration class for sparse embedding model training"""
    
    def __init__(self):
        # Model Configuration
        self.base_model = "distilbert/distilbert-base-uncased"  # Change this to your preferred base model
        self.architecture = "splade"  # Options: "splade", "inference_free_splade", "csr"
        self.max_seq_length = 512
        
        # Dataset Configuration
        self.dataset_name = "sentence-transformers/natural-questions"  # Change to your dataset
        self.dataset_split = "train"
        self.max_samples = 50000  # Limit for quick training, set to None for full dataset
        self.test_size = 1000
        self.eval_size = 1000
        
        # Training Configuration
        self.output_dir = "models/sparse-encoder-finetuned"
        self.run_name = "sparse-encoder-training"
        self.num_train_epochs = 1
        self.per_device_train_batch_size = 16
        self.per_device_eval_batch_size = 16
        self.learning_rate = 2e-5
        self.warmup_ratio = 0.1
        self.fp16 = True  # Set to False if GPU doesn't support FP16
        self.bf16 = False  # Set to True for newer GPUs
        
        # Loss Configuration
        self.loss_type = "splade"  # Options: "splade", "csr"
        self.query_regularizer_weight = 5e-5
        self.document_regularizer_weight = 3e-5
        
        # Evaluation Configuration
        self.eval_strategy = "steps"
        self.eval_steps = 1000
        self.save_steps = 1000
        self.logging_steps = 100
        
        # Hugging Face Configuration
        self.push_to_hub = True
        self.hub_model_id = None  # Will be set automatically or specify custom name
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert config to dictionary"""
        return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}

# Initialize configuration
config = TrainingConfig()
print("‚úÖ Configuration initialized!")
print(f"Training configuration: {config.to_dict()}")

# =============================================================================
# SECTION 3: HUGGING FACE AUTHENTICATION
# =============================================================================

def setup_huggingface_auth():
    """Setup Hugging Face authentication for pushing models"""
    try:
        # Try to get token from environment or prompt user
        hf_token = os.getenv('HF_TOKEN')
        if not hf_token:
            print("Please enter your Hugging Face token:")
            hf_token = input("HF Token: ").strip()
        
        login(token=hf_token)
        print("‚úÖ Successfully authenticated with Hugging Face!")
        return True
    except Exception as e:
        print(f"‚ùå Authentication failed: {e}")
        print("You can skip this and train locally without pushing to hub")
        return False

# Setup authentication if push_to_hub is enabled
if config.push_to_hub:
    auth_success = setup_huggingface_auth()
    if not auth_success:
        config.push_to_hub = False
        print("‚ö†Ô∏è Disabled push_to_hub due to authentication failure")

# =============================================================================
# SECTION 4: MODEL INITIALIZATION
# =============================================================================

def create_splade_model(base_model: str, max_seq_length: int = 512) -> SparseEncoder:
    """Create a SPLADE architecture model"""
    mlm_transformer = MLMTransformer(
        base_model,
        tokenizer_args={"model_max_length": max_seq_length}
    )
    splade_pooling = SpladePooling(pooling_strategy="max")
    
    model = SparseEncoder(modules=[mlm_transformer, splade_pooling])
    print(f"‚úÖ Created SPLADE model from {base_model}")
    return model

def create_inference_free_splade_model(base_model: str, max_seq_length: int = 512) -> SparseEncoder:
    """Create an Inference-free SPLADE architecture model"""
    mlm_transformer = MLMTransformer(
        base_model,
        tokenizer_args={"model_max_length": max_seq_length}
    )
    splade_pooling = SpladePooling(pooling_strategy="max")
    
    router = Router.for_query_document(
        query_modules=[SparseStaticEmbedding(tokenizer=mlm_transformer.tokenizer, frozen=False)],
        document_modules=[mlm_transformer, splade_pooling],
    )
    
    model = SparseEncoder(modules=[router], similarity_fn_name="dot")
    print(f"‚úÖ Created Inference-free SPLADE model from {base_model}")
    return model

def create_csr_model(base_model: str) -> SparseEncoder:
    """Create a CSR (Contrastive Sparse Representation) architecture model"""
    # This works with dense embedding models
    model = SparseEncoder(base_model)
    print(f"‚úÖ Created CSR model from {base_model}")
    return model

def initialize_model(config: TrainingConfig) -> SparseEncoder:
    """Initialize model based on configuration"""
    if config.architecture == "splade":
        model = create_splade_model(config.base_model, config.max_seq_length)
    elif config.architecture == "inference_free_splade":
        model = create_inference_free_splade_model(config.base_model, config.max_seq_length)
    elif config.architecture == "csr":
        model = create_csr_model(config.base_model)
    else:
        raise ValueError(f"Unknown architecture: {config.architecture}")
    
    # Add model card data
    model.model_card_data = SparseEncoderModelCardData(
        language="en",
        license="apache-2.0",
        model_name=f"{config.architecture} model fine-tuned on {config.dataset_name}",
    )
    
    return model

# Initialize the model
model = initialize_model(config)
print(f"Model architecture:\n{model}")

# =============================================================================
# SECTION 5: DATASET LOADING AND PREPARATION
# =============================================================================

def load_and_prepare_dataset(config: TrainingConfig):
    """Load and prepare dataset for training"""
    print(f"Loading dataset: {config.dataset_name}")
    
    # Load the full dataset
    if config.max_samples:
        full_dataset = load_dataset(config.dataset_name, split=f"{config.dataset_split}[:{config.max_samples}]")
    else:
        full_dataset = load_dataset(config.dataset_name, split=config.dataset_split)
    
    print(f"Loaded {len(full_dataset)} samples")
    print(f"Dataset features: {full_dataset.features}")
    print(f"Sample data: {full_dataset[0]}")
    
    # Split dataset
    total_eval_size = config.test_size + config.eval_size
    dataset_dict = full_dataset.train_test_split(test_size=total_eval_size, seed=42)
    
    # Further split test into eval and test
    eval_test_dict = dataset_dict["test"].train_test_split(
        test_size=config.test_size, 
        train_size=config.eval_size, 
        seed=42
    )
    
    train_dataset = dataset_dict["train"]
    eval_dataset = eval_test_dict["train"]  # This is our eval set
    test_dataset = eval_test_dict["test"]   # This is our final test set
    
    print(f"‚úÖ Dataset split complete:")
    print(f"  - Training samples: {len(train_dataset)}")
    print(f"  - Evaluation samples: {len(eval_dataset)}")
    print(f"  - Test samples: {len(test_dataset)}")
    
    return train_dataset, eval_dataset, test_dataset

# Load datasets
train_dataset, eval_dataset, test_dataset = load_and_prepare_dataset(config)

# =============================================================================
# SECTION 6: LOSS FUNCTION SETUP
# =============================================================================

def setup_loss_function(model: SparseEncoder, config: TrainingConfig):
    """Setup loss function based on configuration"""
    
    # Base loss function
    base_loss = SparseMultipleNegativesRankingLoss(model=model)
    
    if config.loss_type == "splade" or config.architecture in ["splade", "inference_free_splade"]:
        loss = SpladeLoss(
            model=model,
            loss=base_loss,
            query_regularizer_weight=config.query_regularizer_weight,
            document_regularizer_weight=config.document_regularizer_weight,
        )
    elif config.loss_type == "csr" or config.architecture == "csr":
        loss = CSRLoss(
            model=model,
            loss=base_loss,
            l1_regularizer_weight=1e-4,
            l2_regularizer_weight=1e-4,
        )
    else:
        raise ValueError(f"Unknown loss type: {config.loss_type}")
    
    print(f"‚úÖ Loss function setup: {type(loss).__name__}")
    return loss

# Setup loss function
loss = setup_loss_function(model, config)

# =============================================================================
# SECTION 7: TRAINING ARGUMENTS AND EVALUATOR SETUP
# =============================================================================

def setup_training_arguments(config: TrainingConfig) -> SparseEncoderTrainingArguments:
    """Setup training arguments"""
    
    args_dict = {
        "output_dir": config.output_dir,
        "num_train_epochs": config.num_train_epochs,
        "per_device_train_batch_size": config.per_device_train_batch_size,
        "per_device_eval_batch_size": config.per_device_eval_batch_size,
        "learning_rate": config.learning_rate,
        "warmup_ratio": config.warmup_ratio,
        "fp16": config.fp16,
        "bf16": config.bf16,
        "batch_sampler": BatchSamplers.NO_DUPLICATES,
        "eval_strategy": config.eval_strategy,
        "eval_steps": config.eval_steps,
        "save_strategy": "steps",
        "save_steps": config.save_steps,
        "save_total_limit": 2,
        "logging_steps": config.logging_steps,
        "run_name": config.run_name,
        "load_best_model_at_end": True,
        "metric_for_best_model": "eval_loss",
        "greater_is_better": False,
    }
    
    # Add router mapping for inference-free SPLADE
    if config.architecture == "inference_free_splade":
        # Determine column names from dataset
        sample = train_dataset[0]
        columns = list(sample.keys())
        
        if len(columns) >= 2:
            args_dict["router_mapping"] = {
                columns[0]: "query",
                columns[1]: "document",
            }
            
        # Higher learning rate for SparseStaticEmbedding
        args_dict["learning_rate_mapping"] = {
            r"SparseStaticEmbedding\.weight": 1e-3,
        }
    
    args = SparseEncoderTrainingArguments(**args_dict)
    print(f"‚úÖ Training arguments configured")
    return args

def setup_evaluator():
    """Setup evaluator for model evaluation"""
    try:
        # Use NanoMSMARCO for quick evaluation
        evaluator = SparseNanoBEIREvaluator(
            dataset_names=["msmarco"],
            batch_size=16
        )
        print("‚úÖ NanoBEIR evaluator setup complete")
        return evaluator
    except Exception as e:
        print(f"‚ö†Ô∏è Could not setup NanoBEIR evaluator: {e}")
        print("Training will proceed without evaluator")
        return None

# Setup training arguments and evaluator
training_args = setup_training_arguments(config)
evaluator = setup_evaluator()

# =============================================================================
# SECTION 8: TRAINER SETUP AND TRAINING
# =============================================================================

def train_model(
    model: SparseEncoder,
    train_dataset: Dataset,
    eval_dataset: Dataset,
    loss,
    training_args: SparseEncoderTrainingArguments,
    evaluator=None
):
    """Train the sparse encoder model"""
    
    print("üöÄ Starting model training...")
    
    # Create trainer
    trainer = SparseEncoderTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        loss=loss,
        evaluator=evaluator,
    )
    
    # Train the model
    trainer.train()
    
    print("‚úÖ Training completed!")
    return trainer

# Train the model
trainer = train_model(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    loss=loss,
    training_args=training_args,
    evaluator=evaluator
)

# =============================================================================
# SECTION 9: MODEL EVALUATION
# =============================================================================

def evaluate_model(model: SparseEncoder, test_dataset: Dataset, evaluator=None):
    """Evaluate the trained model"""
    print("üìä Evaluating trained model...")
    
    # Basic evaluation using the trainer
    if hasattr(trainer, 'evaluate'):
        eval_results = trainer.evaluate(test_dataset)
        print(f"Test evaluation results: {eval_results}")
    
    # Detailed evaluation using evaluator
    if evaluator:
        try:
            detailed_results = evaluator(model)
            print(f"Detailed evaluation results: {detailed_results}")
        except Exception as e:
            print(f"‚ö†Ô∏è Detailed evaluation failed: {e}")
    
    # Test model inference
    print("\nüîç Testing model inference...")
    test_sentences = [
        "What is the capital of France?",
        "How does photosynthesis work?",
        "Explain machine learning in simple terms"
    ]
    
    embeddings = model.encode(test_sentences[:2])
    print(f"Embedding shape: {embeddings.shape}")
    
    # Calculate similarity
    similarities = model.similarity(embeddings, embeddings)
    print(f"Similarity matrix:\n{similarities}")
    
    # Decode embeddings for interpretation (works with SPLADE models)
    try:
        decoded = model.decode(embeddings, top_k=5)
        for sentence, decoded_tokens in zip(test_sentences[:2], decoded):
            print(f"\nSentence: {sentence}")
            print(f"Top tokens: {decoded_tokens}")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not decode embeddings: {e}")

# Evaluate the model
evaluate_model(model, test_dataset, evaluator)

# =============================================================================
# SECTION 10: MODEL SAVING AND PUSHING TO HUB
# =============================================================================

def save_and_push_model(
    model: SparseEncoder, 
    config: TrainingConfig, 
    trainer: SparseEncoderTrainer
):
    """Save model locally and push to Hugging Face Hub"""
    
    # Save model locally
    final_model_path = f"{config.output_dir}/final"
    model.save_pretrained(final_model_path)
    print(f"‚úÖ Model saved locally to: {final_model_path}")
    
    # Push to Hub if enabled
    if config.push_to_hub:
        try:
            # Determine hub model ID
            if config.hub_model_id:
                hub_model_id = config.hub_model_id
            else:
                # Create automatic name based on config
                hub_model_id = f"sparse-{config.architecture}-{config.base_model.split('/')[-1]}-{config.dataset_name.split('/')[-1]}"
                hub_model_id = hub_model_id.replace('_', '-')
            
            print(f"üöÄ Pushing model to Hub as: {hub_model_id}")
            model.push_to_hub(hub_model_id)
            print(f"‚úÖ Model successfully pushed to: https://huggingface.co/{hub_model_id}")
            
            return hub_model_id
            
        except Exception as e:
            print(f"‚ùå Failed to push to Hub: {e}")
            print("Model is still saved locally and can be used")
            return None
    
    return None

# Save and push model
hub_model_id = save_and_push_model(model, config, trainer)

# =============================================================================
# SECTION 11: USAGE EXAMPLE
# =============================================================================

def demonstrate_model_usage(model_path_or_id: str):
    """Demonstrate how to use the trained model"""
    print(f"\nüéØ Model Usage Example")
    print("="*50)
    
    # Load the model
    if hub_model_id:
        demo_model = SparseEncoder(hub_model_id)
        print(f"Loaded model from Hub: {hub_model_id}")
    else:
        demo_model = SparseEncoder(f"{config.output_dir}/final")
        print(f"Loaded model from local path: {config.output_dir}/final")
    
    # Example usage
    queries = [
        "How to train machine learning models?",
        "What is natural language processing?"
    ]
    
    documents = [
        "Machine learning training involves feeding data to algorithms to learn patterns.",
        "Natural language processing is a field of AI that focuses on human language.",
        "Deep learning uses neural networks with multiple layers.",
        "Computer vision enables machines to interpret visual information."
    ]
    
    print(f"\nQueries: {queries}")
    print(f"Documents: {documents}")
    
    # Encode queries and documents
    query_embeddings = model.encode_query(queries, convert_to_sparse_tensor=True)
    doc_embeddings = model.encode_document(documents, convert_to_sparse_tensor=True)
    
    print(f"\nQuery embeddings shape: {query_embeddings.shape}")
    print(f"Document embeddings shape: {doc_embeddings.shape}")
    
    # Calculate similarities
    similarities = model.similarity(query_embeddings, doc_embeddings)
    print(f"\nSimilarity matrix shape: {similarities.shape}")
    
    # Show top matches for each query
    for i, query in enumerate(queries):
        print(f"\nQuery: {query}")
        query_similarities = similarities[i]
        top_indices = torch.topk(query_similarities, k=2).indices
        
        for j, doc_idx in enumerate(top_indices):
            score = query_similarities[doc_idx].item()
            print(f"  Rank {j+1}: (Score: {score:.4f}) {documents[doc_idx]}")

# Demonstrate usage
demonstrate_model_usage(hub_model_id or f"{config.output_dir}/final")

# =============================================================================
# SECTION 12: SUMMARY AND NEXT STEPS
# =============================================================================

print("\n" + "="*80)
print("üéâ TRAINING COMPLETE!")
print("="*80)

print(f"\nüìã Training Summary:")
print(f"  - Architecture: {config.architecture}")
print(f"  - Base Model: {config.base_model}")
print(f"  - Dataset: {config.dataset_name}")
print(f"  - Training Samples: {len(train_dataset)}")
print(f"  - Epochs: {config.num_train_epochs}")
print(f"  - Final Model Path: {config.output_dir}/final")

if hub_model_id:
    print(f"  - Hub Model ID: {hub_model_id}")
    print(f"  - Hub URL: https://huggingface.co/{hub_model_id}")

print(f"\nüîß Model Configuration:")
for key, value in config.to_dict().items():
    print(f"  - {key}: {value}")

print(f"\nüöÄ Next Steps:")
print("1. Evaluate your model on downstream tasks")
print("2. Consider training with more data or epochs for better performance")
print("3. Experiment with different architectures or hyperparameters")
print("4. Integrate with vector databases for production deployment")
print("5. Try hybrid search combining sparse and dense embeddings")

if config.architecture == "inference_free_splade":
    print("\nüí° Inference-free SPLADE Benefits:")
    print("- Near-instant query processing")
    print("- Reduced computational costs")
    print("- Suitable for real-time applications")

print(f"\nüìö Documentation:")
print("- Sentence Transformers: https://sbert.net/")
print("- Sparse Embeddings Guide: https://sbert.net/docs/sentence_transformer/training_overview.html")
print("- Vector Databases: https://sbert.net/examples/applications/semantic-search/README.html")

print("\n‚ú® Happy training! ‚ú®")
